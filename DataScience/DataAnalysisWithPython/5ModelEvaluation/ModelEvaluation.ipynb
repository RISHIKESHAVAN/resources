{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Model Evaluation tells us how our model preforms in the real world. In-sample evaluation tells us how well our model fits the data already given to train it. It does not give us an estimate of how well the trained model can predict new data. The solution is to split our data up, use the In-sample data or training data to train the model. The rest of the data called test data is used as out-of-sample data. This data is then used to approximate how the model preforms in the real world.\n",
    "\n",
    "We use the test data to get an idea how our model will perform in the real world. When we split a data set, usually the larger portion of data (70%) is used for training and a smaller part (30%) is used for testing.\n",
    "\n",
    "We use a training set to build a model and discover predictive relationships. We then use a testing set to evaluate model performance. When we have completed testing our model, we should use all the data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"train test split\" function randomly splits a dataset into training and testing subsets:\n",
    "> xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=0.3, random_state=0)\n",
    "\n",
    "**yData** - is the target variable (\"price\" in the car appraisal example)\n",
    "\n",
    "**xDdata** - the list of predictor variables. (all the other variables in the car data set that we use to predict the price.)\n",
    "\n",
    "The output is an array: \n",
    "- \"xTrain\" and \"yTrain\" - the subsets for training\n",
    "- \"xTest\" and \"yTest\" - the subsets for testing. \n",
    "- \"test_size\" - percentage of the data for the testing set (30%). \n",
    "- random_state - a random seed for random dataset splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization error\n",
    "Generalization error is a measure of how well our data does at predicting previously unseen data. The error we obtain using our testing data is an approximation of this error.\n",
    "\n",
    "Using a lot of data for training gives us an accurate means of determining how our model will perform in the real world, but the precision of the performance will be low. If we use fewer data points to train the model and more to test the model, the accuracy of the generalization performance will be less, but the model will have good precision.\n",
    "\n",
    "To overcome this problem, we use cross validation. It is one of the most common 'out-of-sample evaluation metrics'. In this method, the dataset is split into k-equal groups; each group is referred to as a fold. For example 4 folds. Some of the folds can be used as a training set, which we use to train the model, and the remaining parts are used as a test set, which we use to test the model. For example, we can use three folds for training; then use one fold for testing. This is repeated until each partition is used for both training and testing. At the end, we use the average results as the estimate of out-of-sample error.\n",
    "The evaluation metric depends on the model. For example, the R-squared.\n",
    "> scores = cross_val_score(lr, xData, yData, cv=3)\n",
    "\n",
    "- lr -  the type of model we are using to do the cross validation.\n",
    "- x_data - the predictor variable data\n",
    "- y_data - the target variable data\n",
    "- cv - to manage the number of partitions. Here, cv = 3, which means the data set is split into 3 equal partitions.\n",
    "\n",
    "The function returns an array of scores, one for each partition that was chosen as the testing set.\n",
    "We can average the result together to estimate out-of-sample R-squared using the mean function in numpy.\n",
    "\n",
    "`np.mean(scores)`\n",
    "\n",
    "What if we want a little more information: what if we want to know the actual predicted values supplied by our model before the R squared values are calculated? To do this, we use the cross_val_predict() function.\n",
    "> yHat = cross_val_predict(lr2e, xData, yData, cv=3)\n",
    "\n",
    "The inputs are the same, but the output is a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting and Model Selection:\n",
    "- Underfitting -  where the model is too simple to fit the data.\n",
    "- Overfitting - where the model is too flexible and fits the noise rather than the function.\n",
    "\n",
    "Plotting a graph of R^2 and the order of the polynomial for the train and test data will help us get select the right model\n",
    "\n",
    "We can calculate different R-squared values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [1,2,3,4] # We create a list containing different polynomial orders.\n",
    "for n in order:\n",
    "    pr = PolynomialFeatures(degree = n) # We create a polynomial feature object with the order of the polynomial as a parameter \n",
    "    xTrainPr = pr.fit_transform(xTrain[\"horsepower\"]) # We transform the training and test data into a polynomial \n",
    "    xTestPr = pr.fit_transform(xTest[\"horsepower\"])\n",
    "    lr.fit(xTrainPr, yTrain) # We fit the regression model using the transformed data.\n",
    "    RsquTest.append(lr.score(xTestPr,yTest)) # We then calculate the R-squared using the test data and store it in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression:\n",
    "Higher degree polynomials have coefficients of very large magnitude. Ridge regression controls the magnitude of these polynomial coefficients by introducing the parameter alpha. Alpha is a parameter we select before fitting or training the model. As alpha increases, the parameters get smaller. This is most evident for the higher order polynomial features, but alpha must be selected carefully. If alpha is too large, the coefficients will approach zero and under-fit the data. If alpha is zero, the over-fitting is evident.\n",
    "\n",
    "In order to select alpha we use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeModel = Ridge(alpha=0.1)\n",
    "ridgeModel.fit(x,y)\n",
    "ridgeModel.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the parameter alpha, we use some data for training. We use a second set called validation data; this is similar to test data, but it is used to select parameters like alpha. We start with a small value of alpha, we train the model, make a prediction using the validation data, then calculate the R squared and store the values. Repeat the value for a larger value of alpha. We train the model again, make a prediction using the validation data, then calculate the R squared and store the values of R squared.\n",
    "\n",
    "We repeat the process for a different alpha value, training the model, and making a prediction. We select the value of alpha that maximizes the R squared. \n",
    "\n",
    "Note that we can use other metrics to select the value of alpha like mean squared error.\n",
    "\n",
    "The Overfitting problem is even worse if we have lots of features. There are chances that increasing the alpha gives a higher r squared for the validation data. But the same alpha value  might produce a lower r squared in the training data because it prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search:\n",
    "Grid search allows us to scan through multiple free parameters with few lines of code. Parameters like the alpha term discussed in the previous video are not part of the fitting or training process. These values are called hyperparameters. Scikit-learn has a means of automatically iterating over these hyperparameters using cross-validation. This method is called Grid search. \n",
    "\n",
    "Grid search takes the model or objects you would like to train and different values of the hyperparameters.\n",
    "It then calculates the mean square error or R squared for various hyperparameter values, allowing you to choose the best values. Let the small circles represent different hyperparameters.\n",
    "\n",
    "To select the hyperparameter, we split our dataset into three parts, the training set, validation set, and test set. We train the model for different hyperparameters. We use the R squared or mean square error for each model. We select the hyperparameter that minimizes the mean squared error or maximizes the R squared on the validation set. We finally test our model performance using the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
